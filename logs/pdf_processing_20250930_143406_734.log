2025-09-30 14:34:06,735 |     INFO |             __init__:  60 | === Starting PDF Processing Run: 20250930_143406_734 ===
2025-09-30 14:34:06,740 |     INFO |            remap_pdf:  82 | Starting PDF remapping with 1 mappings in overlay mode
2025-09-30 14:34:06,740 |     INFO |        log_input_pdf:  67 | Input PDF saved: /Users/ashishrajshekhar/codex_code_glyph/runs/20250930_143406_734/input.pdf (100635 bytes)
2025-09-30 14:34:06,741 |     INFO |   log_mode_selection:  76 | Processing mode selected: overlay
2025-09-30 14:34:06,741 |     INFO |         log_mappings:  81 | Word mappings to apply: 1 total
2025-09-30 14:34:06,741 |     INFO |         log_mappings:  83 |   'benefit' → 'flaw'
2025-09-30 14:34:06,741 |     INFO |         log_mappings:  81 | Word mappings to apply: 1 total
2025-09-30 14:34:06,741 |     INFO |         log_mappings:  83 |   'benefit' → 'flaw'
2025-09-30 14:34:06,757 |     INFO | log_pattern_building:  99 | Building regex pattern for 1 words (ignore_case=True)
2025-09-30 14:34:06,757 |    DEBUG | log_pattern_building: 100 | Words to match: ['benefit']
2025-09-30 14:34:06,757 |    DEBUG | log_pattern_building: 101 | Compiled pattern: benefit
2025-09-30 14:34:06,763 |     INFO | _apply_overlay_mode_mapping: 300 | Processing page 1 with 164 operations
2025-09-30 14:34:06,763 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'RNNs and LSTM Quiz'
2025-09-30 14:34:06,763 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'Multiple Choice Questions'
2025-09-30 14:34:06,763 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'Q1.'
2025-09-30 14:34:06,763 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'What is the primary benefit of stacking multiple RNN layers (i.e., stacked RNNs)?'
2025-09-30 14:34:06,763 |     INFO |     process_tj_array:  77 | TJ Array - Applied 1 replacements
2025-09-30 14:34:06,763 |     INFO |     process_tj_array:  79 |   'benefit' → 'flaw' at positions 20-27
2025-09-30 14:34:06,763 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'A. Faster training'
2025-09-30 14:34:06,763 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'B. Lower memory usage'
2025-09-30 14:34:06,763 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'C. Better learning of hierarchical features'
2025-09-30 14:34:06,763 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'D. Simpler architecture'
2025-09-30 14:34:06,763 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'Q2.'
2025-09-30 14:34:06,763 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'Which of the following is the main reason RNNs struggle with long-term dependencies?'
2025-09-30 14:34:06,764 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'A. Overfitting'
2025-09-30 14:34:06,764 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'B. Vanishing gradients'
2025-09-30 14:34:06,764 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'C. Lack of non-linearity'
2025-09-30 14:34:06,764 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'D. Insuffcient data'
2025-09-30 14:34:06,764 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'Q3.'
2025-09-30 14:34:06,764 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'What diflerentiates an LSTM cell from a standard RNN cell?'
2025-09-30 14:34:06,764 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'A. It uses ReLU instead of tanh'
2025-09-30 14:34:06,764 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'B. It introduces gates to control the ow of information'
2025-09-30 14:34:06,764 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'C. It has fewer parameters'
2025-09-30 14:34:06,764 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'D. It is a convolutional architecture'
2025-09-30 14:34:06,764 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'Q4.'
2025-09-30 14:34:06,764 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'In a standard LSTM, which gate is responsible for deciding how much of the past memory to keep?'
2025-09-30 14:34:06,764 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'A. Output gate'
2025-09-30 14:34:06,764 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'B. Forget gate'
2025-09-30 14:34:06,764 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'C. Input gate'
2025-09-30 14:34:06,764 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'D. Update gate'
2025-09-30 14:34:06,764 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'Descriptive Questions'
2025-09-30 14:34:06,764 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'Q5.'
2025-09-30 14:34:06,765 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'Why is the forget gate bias in LSTMs often initialized to a high value (e.g., 2 or 3)? Explain its eflect on long-term'
2025-09-30 14:34:06,765 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'dependency learning.'
2025-09-30 14:34:06,765 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'Q6.'
2025-09-30 14:34:06,765 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'Bidirectional RNNs are often used for POS tagging but not machine translation. Explain why, considering input-output'
2025-09-30 14:34:06,765 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'alignment and context ow.'
2025-09-30 14:34:06,765 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'Q7.'
2025-09-30 14:34:06,765 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'Designing an RNN model for variable-length legal documents with long dependencies:'
2025-09-30 14:34:06,765 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: '(a) Choose between vanilla RNN or LSTM.'
2025-09-30 14:34:06,765 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: '(b) Stack layers or keep it shallow?'
2025-09-30 14:34:06,765 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: '(c) Make it bidirectional?'
2025-09-30 14:34:06,765 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'Justify each choice based on model behavior and task needs.'
2025-09-30 14:34:06,765 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'Q8.'
2025-09-30 14:34:06,765 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'Consider a vanilla RNN with recurrent weight matrix'
2025-09-30 14:34:06,765 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'W'
2025-09-30 14:34:06,765 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'h'
2025-09-30 14:34:06,765 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'and sequence length 50. Analyze gradient behavior:'
2025-09-30 14:34:06,765 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: '(1) If'
2025-09-30 14:34:06,765 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'k'
2025-09-30 14:34:06,765 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'W'
2025-09-30 14:34:06,766 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'h'
2025-09-30 14:34:06,766 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'k'
2025-09-30 14:34:06,766 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: '= 0'
2025-09-30 14:34:06,766 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: ':'
2025-09-30 14:34:06,766 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: '9: Will gradients vanish or explode? Justify.'
2025-09-30 14:34:06,766 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: '(2) If'
2025-09-30 14:34:06,766 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'k'
2025-09-30 14:34:06,766 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'W'
2025-09-30 14:34:06,766 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'h'
2025-09-30 14:34:06,766 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'k'
2025-09-30 14:34:06,766 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: '= 1'
2025-09-30 14:34:06,766 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: ':'
2025-09-30 14:34:06,766 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: '2: Will gradients vanish or explode? Justify. Suggest an easy fix and explain how it helps.'
2025-09-30 14:34:06,766 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'Hint: Consider eigenvalue eflects on gradient propagation over time.'
2025-09-30 14:34:06,766 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: '1'
2025-09-30 14:34:06,766 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 0: 'RNNs and LSTM Quiz Multiple Choice Questions Q1.'
2025-09-30 14:34:06,767 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 1: 'Multiple Choice Questions Q1. What is the primary flaw of stacking multiple RNN layers (i.e., stacked RNNs)?'
2025-09-30 14:34:06,767 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 2: 'Q1. What is the primary flaw of stacking multiple RNN layers (i.e., stacked RNNs)? A. Faster training'
2025-09-30 14:34:06,767 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 3: 'What is the primary flaw of stacking multiple RNN layers (i.e., stacked RNNs)? A. Faster training B. Lower memory usage'
2025-09-30 14:34:06,767 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 4: 'A. Faster training B. Lower memory usage C. Better learning of hierarchical features'
2025-09-30 14:34:06,767 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 5: 'B. Lower memory usage C. Better learning of hierarchical features D. Simpler architecture'
2025-09-30 14:34:06,767 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 6: 'C. Better learning of hierarchical features D. Simpler architecture Q2.'
2025-09-30 14:34:06,767 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 7: 'D. Simpler architecture Q2. Which of the following is the main reason RNNs struggle with long-term dependencies?'
2025-09-30 14:34:06,767 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 8: 'Q2. Which of the following is the main reason RNNs struggle with long-term dependencies? A. Overfitting'
2025-09-30 14:34:06,767 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 9: 'Which of the following is the main reason RNNs struggle with long-term dependencies? A. Overfitting B. Vanishing gradients'
2025-09-30 14:34:06,767 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 10: 'A. Overfitting B. Vanishing gradients C. Lack of non-linearity'
2025-09-30 14:34:06,768 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 11: 'B. Vanishing gradients C. Lack of non-linearity D. Insuffcient data'
2025-09-30 14:34:06,768 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 12: 'C. Lack of non-linearity D. Insuffcient data Q3.'
2025-09-30 14:34:06,768 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 13: 'D. Insuffcient data Q3. What diflerentiates an LSTM cell from a standard RNN cell?'
2025-09-30 14:34:06,768 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 14: 'Q3. What diflerentiates an LSTM cell from a standard RNN cell? A. It uses ReLU instead of tanh'
2025-09-30 14:34:06,768 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 15: 'What diflerentiates an LSTM cell from a standard RNN cell? A. It uses ReLU instead of tanh B. It introduces gates to control the ow of information'
2025-09-30 14:34:06,768 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 16: 'A. It uses ReLU instead of tanh B. It introduces gates to control the ow of information C. It has fewer parameters'
2025-09-30 14:34:06,768 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 17: 'B. It introduces gates to control the ow of information C. It has fewer parameters D. It is a convolutional architecture'
2025-09-30 14:34:06,768 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 18: 'C. It has fewer parameters D. It is a convolutional architecture Q4.'
2025-09-30 14:34:06,768 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 19: 'D. It is a convolutional architecture Q4. In a standard LSTM, which gate is responsible for deciding how much of the past memory to keep?'
2025-09-30 14:34:06,768 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 20: 'Q4. In a standard LSTM, which gate is responsible for deciding how much of the past memory to keep? A. Output gate'
2025-09-30 14:34:06,769 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 21: 'In a standard LSTM, which gate is responsible for deciding how much of the past memory to keep? A. Output gate B. Forget gate'
2025-09-30 14:34:06,769 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 22: 'A. Output gate B. Forget gate C. Input gate'
2025-09-30 14:34:06,769 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 23: 'B. Forget gate C. Input gate D. Update gate'
2025-09-30 14:34:06,769 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 24: 'C. Input gate D. Update gate Descriptive Questions'
2025-09-30 14:34:06,769 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 25: 'D. Update gate Descriptive Questions Q5.'
2025-09-30 14:34:06,769 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 26: 'Descriptive Questions Q5. Why is the forget gate bias in LSTMs often initialized to a high value (e.g., 2 or 3)? Explain its eflect on long-term'
2025-09-30 14:34:06,769 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 27: 'Q5. Why is the forget gate bias in LSTMs often initialized to a high value (e.g., 2 or 3)? Explain its eflect on long-term dependency learning.'
2025-09-30 14:34:06,769 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 28: 'Why is the forget gate bias in LSTMs often initialized to a high value (e.g., 2 or 3)? Explain its eflect on long-term dependency learning. Q6.'
2025-09-30 14:34:06,769 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 29: 'dependency learning. Q6. Bidirectional RNNs are often used for POS tagging but not machine translation. Explain why, considering input-output'
2025-09-30 14:34:06,769 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 30: 'Q6. Bidirectional RNNs are often used for POS tagging but not machine translation. Explain why, considering input-output alignment and context ow.'
2025-09-30 14:34:06,769 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 31: 'Bidirectional RNNs are often used for POS tagging but not machine translation. Explain why, considering input-output alignment and context ow. Q7.'
2025-09-30 14:34:06,770 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 32: 'alignment and context ow. Q7. Designing an RNN model for variable-length legal documents with long dependencies:'
2025-09-30 14:34:06,770 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 33: 'Q7. Designing an RNN model for variable-length legal documents with long dependencies: (a) Choose between vanilla RNN or LSTM.'
2025-09-30 14:34:06,770 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 34: 'Designing an RNN model for variable-length legal documents with long dependencies: (a) Choose between vanilla RNN or LSTM. (b) Stack layers or keep it shallow?'
2025-09-30 14:34:06,770 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 35: '(a) Choose between vanilla RNN or LSTM. (b) Stack layers or keep it shallow? (c) Make it bidirectional?'
2025-09-30 14:34:06,770 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 36: '(b) Stack layers or keep it shallow? (c) Make it bidirectional? Justify each choice based on model behavior and task needs.'
2025-09-30 14:34:06,770 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 37: '(c) Make it bidirectional? Justify each choice based on model behavior and task needs. Q8.'
2025-09-30 14:34:06,770 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 38: 'Justify each choice based on model behavior and task needs. Q8. Consider a vanilla RNN with recurrent weight matrix'
2025-09-30 14:34:06,770 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 39: 'Q8. Consider a vanilla RNN with recurrent weight matrix W'
2025-09-30 14:34:06,770 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 40: 'Consider a vanilla RNN with recurrent weight matrix W h'
2025-09-30 14:34:06,770 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 41: 'W h and sequence length 50. Analyze gradient behavior:'
2025-09-30 14:34:06,771 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 42: 'h and sequence length 50. Analyze gradient behavior: (1) If'
2025-09-30 14:34:06,771 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 43: 'and sequence length 50. Analyze gradient behavior: (1) If k'
2025-09-30 14:34:06,771 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 44: '(1) If k W'
2025-09-30 14:34:06,771 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 45: 'k W h'
2025-09-30 14:34:06,771 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 46: 'W h k'
2025-09-30 14:34:06,771 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 47: 'h k = 0'
2025-09-30 14:34:06,771 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 48: 'k = 0 :'
2025-09-30 14:34:06,771 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 49: '= 0 : 9: Will gradients vanish or explode? Justify.'
2025-09-30 14:34:06,771 |    DEBUG | _find_split_decimal_patterns: 195 | Found split decimal: '= 0 : 9:' → '0.9:'
2025-09-30 14:34:06,771 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 50: ': 9: Will gradients vanish or explode? Justify. (2) If'
2025-09-30 14:34:06,771 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 51: '9: Will gradients vanish or explode? Justify. (2) If k'
2025-09-30 14:34:06,771 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 52: '(2) If k W'
2025-09-30 14:34:06,771 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 53: 'k W h'
2025-09-30 14:34:06,771 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 54: 'W h k'
2025-09-30 14:34:06,771 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 55: 'h k = 1'
2025-09-30 14:34:06,771 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 56: 'k = 1 :'
2025-09-30 14:34:06,771 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 57: '= 1 : 2: Will gradients vanish or explode? Justify. Suggest an easy fix and explain how it helps.'
2025-09-30 14:34:06,771 |    DEBUG | _find_split_decimal_patterns: 195 | Found split decimal: '= 1 : 2:' → '1.2:'
2025-09-30 14:34:06,771 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 58: ': 2: Will gradients vanish or explode? Justify. Suggest an easy fix and explain how it helps. Hint: Consider eigenvalue eflects on gradient propagation over time.'
2025-09-30 14:34:06,772 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 59: '2: Will gradients vanish or explode? Justify. Suggest an easy fix and explain how it helps. Hint: Consider eigenvalue eflects on gradient propagation over time. 1'
2025-09-30 14:34:06,772 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 60: 'Hint: Consider eigenvalue eflects on gradient propagation over time. 1'
2025-09-30 14:34:06,791 |     INFO |       log_output_pdf: 190 | Output PDF saved: /Users/ashishrajshekhar/codex_code_glyph/runs/20250930_143406_734/output.pdf (108631 bytes)
2025-09-30 14:34:06,791 |     INFO |         finalize_run: 222 | === Run Complete: 20250930_143406_734 (Duration: 0.06s) ===
2025-09-30 14:34:06,791 |     INFO |         finalize_run: 223 | Run directory: /Users/ashishrajshekhar/codex_code_glyph/runs/20250930_143406_734
2025-09-30 14:34:06,791 |     INFO |         finalize_run: 224 | Metadata saved: /Users/ashishrajshekhar/codex_code_glyph/runs/20250930_143406_734/run_metadata.json
2025-09-30 14:34:06,791 |     INFO |         finalize_run: 233 | Run completed successfully with 1 mappings applied
