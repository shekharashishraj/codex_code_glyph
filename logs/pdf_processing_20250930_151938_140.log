2025-09-30 15:19:38,142 |     INFO |             __init__:  60 | === Starting PDF Processing Run: 20250930_151938_140 ===
2025-09-30 15:19:38,143 |     INFO |            remap_pdf:  82 | Starting PDF remapping with 1 mappings in overlay mode
2025-09-30 15:19:38,144 |     INFO |        log_input_pdf:  67 | Input PDF saved: /Users/ashishrajshekhar/codex_code_glyph/runs/20250930_151938_140/input.pdf (34173 bytes)
2025-09-30 15:19:38,144 |     INFO |   log_mode_selection:  76 | Processing mode selected: overlay
2025-09-30 15:19:38,145 |     INFO |         log_mappings:  81 | Word mappings to apply: 1 total
2025-09-30 15:19:38,145 |     INFO |         log_mappings:  83 |   'multi-head' → 'single-head'
2025-09-30 15:19:38,145 |     INFO |         log_mappings:  81 | Word mappings to apply: 1 total
2025-09-30 15:19:38,145 |     INFO |         log_mappings:  83 |   'multi-head' → 'single-head'
2025-09-30 15:19:38,158 |     INFO | log_pattern_building:  99 | Building regex pattern for 1 words (ignore_case=True)
2025-09-30 15:19:38,158 |    DEBUG | log_pattern_building: 100 | Words to match: ['multi-head']
2025-09-30 15:19:38,158 |    DEBUG | log_pattern_building: 101 | Compiled pattern: multi\-head
2025-09-30 15:19:38,163 |     INFO | _apply_overlay_mode_mapping: 302 | Processing page 1 with 95 operations
2025-09-30 15:19:38,164 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'CSE 576 Quiz 7 Transformer Pretraining Quiz'
2025-09-30 15:19:38,164 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: '1. Which of the following best explains how multi-head attention improves contextual'
2025-09-30 15:19:38,164 |     INFO |     process_tj_array:  77 | TJ Array - Applied 1 replacements
2025-09-30 15:19:38,164 |     INFO |     process_tj_array:  79 |   'multi-head' → 'single-head' at positions 44-54
2025-09-30 15:19:38,164 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'understanding in Transformers?'
2025-09-30 15:19:38,165 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'A) By reducing the total number of parameters through parallelization'
2025-09-30 15:19:38,165 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'B) By enforcing uniform attention over the sequence to prevent bias'
2025-09-30 15:19:38,165 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'C) By increasing computation speed through batch-wise attention'
2025-09-30 15:19:38,165 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'D) By enabling diflerent heads to attend to diverse relational patterns across positions'
2025-09-30 15:19:38,165 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: '2. Which component of the Transformer architecture is exclusively utilized in GPT, mak-'
2025-09-30 15:19:38,165 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'ing it more suited for generative tasks?'
2025-09-30 15:19:38,165 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'A) Decoder layers with masked self-attention'
2025-09-30 15:19:38,165 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'B) Encoder layers for input sequence modeling'
2025-09-30 15:19:38,165 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'C) A hybrid encoder-decoder combination'
2025-09-30 15:19:38,165 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'D) A purely feed-forward architecture'
2025-09-30 15:19:38,165 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: '3. What design choice in GPT restricts it from leveraging full bidirectional context, and'
2025-09-30 15:19:38,165 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'what consequence does this have?'
2025-09-30 15:19:38,165 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'A) Encoder-based design; restricts output generation'
2025-09-30 15:19:38,166 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'B) Unidirectional left-to-right ow; limits full context understanding'
2025-09-30 15:19:38,166 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'C) Bidirectional masking; leads to context overfitting'
2025-09-30 15:19:38,166 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'D) Cross-attention dependencies; increase inference latency'
2025-09-30 15:19:38,166 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: '4. Which of the following best characterizes the training objectives that enable BERT to'
2025-09-30 15:19:38,166 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'capture both deep token-level context and inter-sentence semantics?'
2025-09-30 15:19:38,166 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'A) Predicting the next token in a left-to-right fashion using unidirectional context'
2025-09-30 15:19:38,166 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'B) Learning to generate a target sequence from an input sequence in an encoder-'
2025-09-30 15:19:38,166 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'decoder setup'
2025-09-30 15:19:38,166 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'C) Jointly optimizing masked token reconstruction and inter-sentence coherence dis-'
2025-09-30 15:19:38,166 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'crimination'
2025-09-30 15:19:38,166 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'D) Aligning image features with textual descriptions through cross-modal supervision'
2025-09-30 15:19:38,166 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'Short Answer Questions'
2025-09-30 15:19:38,166 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: '5. What are the potential drawbacks of the two-stage process of pretraining on large'
2025-09-30 15:19:38,167 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'corpora followed by fine-tuning on specific tasks in Transformer models?'
2025-09-30 15:19:38,167 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: "6. What are the potential drawbacks of GPT's autoregressive training objective when"
2025-09-30 15:19:38,167 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'applied to tasks requiring holistic understanding of text?'
2025-09-30 15:19:38,167 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: '7. BERT utilizes a masked language model (MLM) during pretraining. What is the'
2025-09-30 15:19:38,167 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'primary challenge associated with the MLM approach, and how does it aflect the'
2025-09-30 15:19:38,167 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: "model's downstream performance?"
2025-09-30 15:19:38,167 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: '8. GPT models are known for their unidirectional (left-to-right) processing. How does'
2025-09-30 15:19:38,167 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'this design choice impact their performance on tasks like text generation compared to'
2025-09-30 15:19:38,167 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: 'tasks like text classification?'
2025-09-30 15:19:38,167 |    DEBUG |     process_tj_array:  70 | TJ Array - Combined text: '1'
2025-09-30 15:19:38,167 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 0: 'CSE 576 Quiz 7 Transformer Pretraining Quiz 1. Which of the following best explains how single-head attention improves contextual understanding in Transformers?'
2025-09-30 15:19:38,168 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 1: '1. Which of the following best explains how single-head attention improves contextual understanding in Transformers? A) By reducing the total number of parameters through parallelization'
2025-09-30 15:19:38,168 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 2: 'understanding in Transformers? A) By reducing the total number of parameters through parallelization B) By enforcing uniform attention over the sequence to prevent bias'
2025-09-30 15:19:38,168 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 3: 'A) By reducing the total number of parameters through parallelization B) By enforcing uniform attention over the sequence to prevent bias C) By increasing computation speed through batch-wise attention'
2025-09-30 15:19:38,168 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 4: 'B) By enforcing uniform attention over the sequence to prevent bias C) By increasing computation speed through batch-wise attention D) By enabling diflerent heads to attend to diverse relational patterns across positions'
2025-09-30 15:19:38,168 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 5: 'C) By increasing computation speed through batch-wise attention D) By enabling diflerent heads to attend to diverse relational patterns across positions 2. Which component of the Transformer architecture is exclusively utilized in GPT, mak-'
2025-09-30 15:19:38,168 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 6: 'D) By enabling diflerent heads to attend to diverse relational patterns across positions 2. Which component of the Transformer architecture is exclusively utilized in GPT, mak- ing it more suited for generative tasks?'
2025-09-30 15:19:38,169 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 7: '2. Which component of the Transformer architecture is exclusively utilized in GPT, mak- ing it more suited for generative tasks? A) Decoder layers with masked self-attention'
2025-09-30 15:19:38,169 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 8: 'ing it more suited for generative tasks? A) Decoder layers with masked self-attention B) Encoder layers for input sequence modeling'
2025-09-30 15:19:38,169 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 9: 'A) Decoder layers with masked self-attention B) Encoder layers for input sequence modeling C) A hybrid encoder-decoder combination'
2025-09-30 15:19:38,169 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 10: 'B) Encoder layers for input sequence modeling C) A hybrid encoder-decoder combination D) A purely feed-forward architecture'
2025-09-30 15:19:38,169 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 11: 'C) A hybrid encoder-decoder combination D) A purely feed-forward architecture 3. What design choice in GPT restricts it from leveraging full bidirectional context, and'
2025-09-30 15:19:38,169 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 12: 'D) A purely feed-forward architecture 3. What design choice in GPT restricts it from leveraging full bidirectional context, and what consequence does this have?'
2025-09-30 15:19:38,169 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 13: '3. What design choice in GPT restricts it from leveraging full bidirectional context, and what consequence does this have? A) Encoder-based design; restricts output generation'
2025-09-30 15:19:38,170 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 14: 'what consequence does this have? A) Encoder-based design; restricts output generation B) Unidirectional left-to-right ow; limits full context understanding'
2025-09-30 15:19:38,170 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 15: 'A) Encoder-based design; restricts output generation B) Unidirectional left-to-right ow; limits full context understanding C) Bidirectional masking; leads to context overfitting'
2025-09-30 15:19:38,170 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 16: 'B) Unidirectional left-to-right ow; limits full context understanding C) Bidirectional masking; leads to context overfitting D) Cross-attention dependencies; increase inference latency'
2025-09-30 15:19:38,170 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 17: 'C) Bidirectional masking; leads to context overfitting D) Cross-attention dependencies; increase inference latency 4. Which of the following best characterizes the training objectives that enable BERT to'
2025-09-30 15:19:38,170 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 18: 'D) Cross-attention dependencies; increase inference latency 4. Which of the following best characterizes the training objectives that enable BERT to capture both deep token-level context and inter-sentence semantics?'
2025-09-30 15:19:38,171 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 19: '4. Which of the following best characterizes the training objectives that enable BERT to capture both deep token-level context and inter-sentence semantics? A) Predicting the next token in a left-to-right fashion using unidirectional context'
2025-09-30 15:19:38,171 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 20: 'capture both deep token-level context and inter-sentence semantics? A) Predicting the next token in a left-to-right fashion using unidirectional context B) Learning to generate a target sequence from an input sequence in an encoder-'
2025-09-30 15:19:38,171 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 21: 'A) Predicting the next token in a left-to-right fashion using unidirectional context B) Learning to generate a target sequence from an input sequence in an encoder- decoder setup'
2025-09-30 15:19:38,171 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 22: 'B) Learning to generate a target sequence from an input sequence in an encoder- decoder setup C) Jointly optimizing masked token reconstruction and inter-sentence coherence dis-'
2025-09-30 15:19:38,171 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 23: 'decoder setup C) Jointly optimizing masked token reconstruction and inter-sentence coherence dis- crimination'
2025-09-30 15:19:38,171 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 24: 'C) Jointly optimizing masked token reconstruction and inter-sentence coherence dis- crimination D) Aligning image features with textual descriptions through cross-modal supervision'
2025-09-30 15:19:38,172 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 25: 'crimination D) Aligning image features with textual descriptions through cross-modal supervision Short Answer Questions'
2025-09-30 15:19:38,172 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 26: 'D) Aligning image features with textual descriptions through cross-modal supervision Short Answer Questions 5. What are the potential drawbacks of the two-stage process of pretraining on large'
2025-09-30 15:19:38,172 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 27: 'Short Answer Questions 5. What are the potential drawbacks of the two-stage process of pretraining on large corpora followed by fine-tuning on specific tasks in Transformer models?'
2025-09-30 15:19:38,172 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 28: "5. What are the potential drawbacks of the two-stage process of pretraining on large corpora followed by fine-tuning on specific tasks in Transformer models? 6. What are the potential drawbacks of GPT's autoregressive training objective when"
2025-09-30 15:19:38,172 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 29: "corpora followed by fine-tuning on specific tasks in Transformer models? 6. What are the potential drawbacks of GPT's autoregressive training objective when applied to tasks requiring holistic understanding of text?"
2025-09-30 15:19:38,172 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 30: "6. What are the potential drawbacks of GPT's autoregressive training objective when applied to tasks requiring holistic understanding of text? 7. BERT utilizes a masked language model (MLM) during pretraining. What is the"
2025-09-30 15:19:38,173 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 31: 'applied to tasks requiring holistic understanding of text? 7. BERT utilizes a masked language model (MLM) during pretraining. What is the primary challenge associated with the MLM approach, and how does it aflect the'
2025-09-30 15:19:38,173 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 32: "7. BERT utilizes a masked language model (MLM) during pretraining. What is the primary challenge associated with the MLM approach, and how does it aflect the model's downstream performance?"
2025-09-30 15:19:38,173 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 33: "primary challenge associated with the MLM approach, and how does it aflect the model's downstream performance? 8. GPT models are known for their unidirectional (left-to-right) processing. How does"
2025-09-30 15:19:38,173 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 34: "model's downstream performance? 8. GPT models are known for their unidirectional (left-to-right) processing. How does this design choice impact their performance on tasks like text generation compared to"
2025-09-30 15:19:38,173 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 35: '8. GPT models are known for their unidirectional (left-to-right) processing. How does this design choice impact their performance on tasks like text generation compared to tasks like text classification?'
2025-09-30 15:19:38,173 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 36: 'this design choice impact their performance on tasks like text generation compared to tasks like text classification? 1'
2025-09-30 15:19:38,173 |    DEBUG | _find_cross_array_matches: 137 | Cross-array window 37: 'tasks like text classification? 1'
2025-09-30 15:19:38,184 |     INFO |       log_output_pdf: 190 | Output PDF saved: /Users/ashishrajshekhar/codex_code_glyph/runs/20250930_151938_140/output.pdf (39954 bytes)
2025-09-30 15:19:38,184 |     INFO |         finalize_run: 222 | === Run Complete: 20250930_151938_140 (Duration: 0.04s) ===
2025-09-30 15:19:38,184 |     INFO |         finalize_run: 223 | Run directory: /Users/ashishrajshekhar/codex_code_glyph/runs/20250930_151938_140
2025-09-30 15:19:38,184 |     INFO |         finalize_run: 224 | Metadata saved: /Users/ashishrajshekhar/codex_code_glyph/runs/20250930_151938_140/run_metadata.json
2025-09-30 15:19:38,184 |     INFO |         finalize_run: 233 | Run completed successfully with 1 mappings applied
