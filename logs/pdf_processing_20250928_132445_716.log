2025-09-28 13:24:45,717 |     INFO |             __init__:  60 | === Starting PDF Processing Run: 20250928_132445_716 ===
2025-09-28 13:24:45,735 |     INFO |  log_text_extraction:  90 | Extracted text (2348 chars): 'CSE 576 Quiz 7\nTransformer Pretraining Quiz\n1. Which of the following best explains how multi-head attention improves contextual\nunderstanding in Transformers?\nA) By reducing the total number of parameters through parallelization\nB) By enforcing uniform attention over the sequence to prevent bias\nC) By increasing computation speed through batch-wise attention\nD) By enabling different heads to attend to diverse relational patterns across positions\n2. Which component of the Transformer architectur...'
2025-09-28 13:24:45,736 |    DEBUG |  log_text_extraction:  95 | Full extracted text saved to: /Users/ashishrajshekhar/codex_code_glyph/runs/20250928_132445_716/extracted_text.txt
