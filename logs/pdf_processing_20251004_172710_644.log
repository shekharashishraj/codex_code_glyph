2025-10-04 17:27:10,645 |     INFO |             __init__:  60 | === Starting PDF Processing Run: 20251004_172710_644 ===
2025-10-04 17:27:10,686 |     INFO |  log_text_extraction:  90 | Extracted text (4000 chars): 'CSE 576: Topics in Natural Language Processing\nDr. Vivek Gupta\nSpring 2025\n07/04/2025\nLanguage Modeling: Attention\nAttention\n• Attention provides a solution to the bottleneck problem.\n• Core idea: on each step of the decoder, use direct connection to the encoder to focus \non a particular part of the source sequence\n• First, we will show via diagram (no equations), then we will show with equations\n37\nThe starting point: mean-pooling for RNNs\n38\n• Starting point: a very basic way of ‘passing infor...'
2025-10-04 17:27:10,686 |    DEBUG |  log_text_extraction:  95 | Full extracted text saved to: /Users/ashishrajshekhar/codex_code_glyph/runs/20251004_172710_644/extracted_text.txt
2025-10-04 17:27:53,608 |     INFO |  log_text_extraction:  90 | Extracted text (4000 chars): 'CSE 576: Topics in Natural Language Processing\nDr. Vivek Gupta\nSpring 2025\n09/04/2025\nLanguage Modeling: \nPretraining\nThe pretraining revolution\nGains from pretrained language models\nPretraining has had a major, tangible impact on how well NLP systems work\nPretraining – scaling unsupervised learning on the internet\nKey ideas in pretraining\n•\nMake sure your model can process large-scale, diverse datasets\n•\nDon’t use labeled data (otherwise you can’t scale!)\n•\nCompute-aware scaling \nWord structure...'
2025-10-04 17:27:53,610 |    DEBUG |  log_text_extraction:  95 | Full extracted text saved to: /Users/ashishrajshekhar/codex_code_glyph/runs/20251004_172710_644/extracted_text.txt
