Backpropagation in Neural 
Nets
How to Train a Neural Net?
Training Data
Output
â€¢
Put in Training inputs, get the output
â€¢
Compare output to correct answers: Look at loss function J
â€¢
Adjust and repeat!
â€¢
Backpropagation tells us how to make a single adjustment using
calculus.
How have we trained before?
â€¢
Gradient Descent!
1.
Make prediction
2.
Calculate Loss
3.
Calculate gradient of the loss function w.r.t. parameters
4.
Update parameters by taking a step in the opposite direction
5.
Iterate
How have we trained before?
â€¢
Gradient Descent!
1.
Make prediction
2.
Calculate Loss
3.
Calculate gradient of the loss function w.r.t. parameters
4.
Update parameters by taking a step in the opposite direction
5.
Iterate
} â€˜The Forward Passâ€™
ğ‘¦1
Feedforward Neural Network
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğœ
ğœ
ğœ
ğœ
ğœ
ğœ
ğœ
ğœ
à·œğ‘¦1
à·œğ‘¦2
à·œğ‘¦3
ğ‘¦2
ğ‘¦3
ğ‘¦1
Forward Propagation
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğœ
ğœ
ğœ
ğœ
ğœ
ğœ
ğœ
ğœ
à·œğ‘¦1
à·œğ‘¦2
à·œğ‘¦3
ğ‘¦2
ğ‘¦3
Input
ğ‘¦1
Forward Propagation
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğœ
ğœ
ğœ
ğœ
ğœ
ğœ
ğœ
ğœ
à·œğ‘¦1
à·œğ‘¦2
à·œğ‘¦3
ğ‘¦2
ğ‘¦3
Calculate each Layer
ğ‘¦1
Forward Propagation
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğœ
ğœ
ğœ
ğœ
ğœ
ğœ
ğœ
ğœ
à·œğ‘¦1
à·œğ‘¦2
à·œğ‘¦3
ğ‘¦2
ğ‘¦3
Output
ğ‘¦1
Forward Propagation
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğœ
ğœ
ğœ
ğœ
ğœ
ğœ
ğœ
ğœ
à·œğ‘¦1
à·œğ‘¦2
à·œğ‘¦3
ğ‘¦2
ğ‘¦3
Evaluate:
ğ½ğ‘¦ğ‘–, à·ğ‘¦ğ‘–
How have we trained before?
â€¢
Gradient Descent!
1.
Make prediction
2.
Calculate Loss
3.
Calculate gradient of the loss function w.r.t. parameters
4.
Update parameters by taking a step in the opposite direction
5.
Iterate
How to calculate gradient?
â—
Chain rule
How to Train a Neural Net?
â€¢
How could we change the weights to make our Loss Function lower?
â€¢
Think of neural net as a function F: X -> Y
â€¢
F is a complex computation involving many weights W_k
â€¢
Given the structure, the weights â€œdefineâ€ the function F (and 
therefore define our model)
â€¢
Loss Function is J(y,F(x))
PS. Loss function may also look like L(y, F(x))
PS. F(x) is the current prediction or y^; y is the ground truth (label)
How to Train a Neural Net?
â€¢
Get
ğœ•ğ½
ğœ•ğ‘Šğ‘˜for every weight in the network.
â€¢
This tells us what direction to adjust each Wk if we want to lower 
our loss function.
â€¢
Make an adjustment and repeat!
ğ‘¦1
Feedforward Neural Network
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğœ
ğœ
ğœ
ğœ
ğœ
ğœ
ğœ
ğœ
à·œğ‘¦1
à·œğ‘¦2
à·œğ‘¦3
ğ‘¦2
ğ‘¦3
ğœ•ğ½ğ‘¦ğ‘–, à·ğ‘¦ğ‘–
ğœ•ğ‘Šğ‘˜
ğ‘Š(1)
ğ‘Š(2)
ğ‘Š(3)
Want:
Calculus to the Rescue
â€¢
Use calculus, chain rule, etc. etc.
â€¢
Functions are chosen to have â€œniceâ€ derivatives
â€¢
Numerical issues to be considered
Punchline
ğœ•ğ½
ğœ•ğ‘Š(2) = (à·œğ‘¦âˆ’ğ‘¦) â‹…ğ‘Š3 â‹…ğœâ€² ğ‘§(3) â‹…ğ‘(2)
ğœ•ğ½
ğœ•ğ‘Š(1) =
à·œğ‘¦âˆ’ğ‘¦â‹…ğ‘Š3 â‹…ğœâ€² ğ‘§(3) â‹…ğ‘Š2 â‹…ğœâ€² ğ‘§2
â‹…ğ‘‹
ğœ•ğ½
ğœ•ğ‘Š(3) = (à·œğ‘¦âˆ’ğ‘¦) â‹…ğ‘(3)
â€¢
Recall that: ğœâ€² ğ‘§= ğœ(ğ‘§)(1 âˆ’ğœ(ğ‘§))
â€¢
Though they appear complex, above are easy to compute! 
ğ‘¦1
Backpropagation
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğœ
ğœ
ğœ
ğœ
ğœ
ğœ
ğœ
ğœ
à·œğ‘¦1
à·œğ‘¦2
à·œğ‘¦3
ğ‘¦2
ğ‘¦3
ğœ•ğ½ğ‘¦ğ‘–, à·ğ‘¦ğ‘–
ğœ•ğ‘Šğ‘˜
ğ‘Š(1)
ğ‘Š(2)
ğ‘Š(3)
Want:
ğ‘¦1
Backpropagation
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğœ
ğœ
ğœ
ğœ
ğœ
ğœ
ğœ
ğœ
à·œğ‘¦1
à·œğ‘¦2
à·œğ‘¦3
ğ‘¦2
ğ‘¦3
ğ‘Š(1)
ğ‘Š(2)
ğœ•ğ½ğ‘¦ğ‘–, à·ğ‘¦ğ‘–
ğœ•ğ‘Š3
ğ‘¦1
Backpropagation
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğœ
ğœ
ğœ
ğœ
ğœ
ğœ
ğœ
ğœ
à·œğ‘¦1
à·œğ‘¦2
à·œğ‘¦3
ğ‘¦2
ğ‘¦3
ğœ•ğ½ğ‘¦ğ‘–, à·ğ‘¦ğ‘–
ğœ•ğ‘Š3
ğœ•ğ½ğ‘¦ğ‘–, à·ğ‘¦ğ‘–
ğœ•ğ‘Š2
ğ‘Š(1)
ğ‘¦1
Backpropagation
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğœ
ğœ
ğœ
ğœ
ğœ
ğœ
ğœ
ğœ
à·œğ‘¦1
à·œğ‘¦2
à·œğ‘¦3
ğ‘¦2
ğ‘¦3
ğœ•ğ½ğ‘¦ğ‘–, à·ğ‘¦ğ‘–
ğœ•ğ‘Š3
ğœ•ğ½ğ‘¦ğ‘–, à·ğ‘¦ğ‘–
ğœ•ğ‘Š2
ğœ•ğ½ğ‘¦ğ‘–, à·ğ‘¦ğ‘–
ğœ•ğ‘Š1
How have we trained before?
â€¢
Gradient Descent!
1.
Make prediction
2.
Calculate Loss
3.
Calculate gradient of the loss function w.r.t. parameters
4.
Update parameters by taking a step in the opposite direction
5.
Iterate
In short, Backpropagation ~= passing back the loss
Vanishing Gradients
ğœ•ğ½
ğœ•ğ‘Š(1) =
à·œğ‘¦âˆ’ğ‘¦â‹…ğ‘Š3 â‹…ğœâ€² ğ‘§(3) â‹…ğ‘Š2 â‹…ğœâ€² ğ‘§2
â‹…ğ‘‹
â€¢
Remember: ğœâ€² ğ‘§= ğœğ‘§
1 âˆ’ğœğ‘§
â‰¤.25
â€¢
As we have more layers, the gradient gets very small at the early 
layers.
â€¢
This is known as the â€œvanishing gradientâ€ problem.
â€¢
For this reason, other activations (such as ReLU) have become more 
common.
Recall that:
Other Activation Functions
Hyperbolic Tangent Function
ğ‘¡ğ‘ğ‘›â„ğ‘§= sinh(ğ‘§)
cosh(ğ‘§) = ğ‘’2ğ‘¥âˆ’1
ğ‘’2ğ‘¥+ 1
â€¢
Hyperbolic tangent function
â€¢
Pronounced â€œtanchâ€
ğ‘¡ğ‘ğ‘›â„0 = 0
ğ‘¡ğ‘ğ‘›â„âˆ= 1
ğ‘¡ğ‘ğ‘›â„âˆ’âˆ= âˆ’1
Hyperbolic Tangent Function
Rectified Linear Unit (ReLU)
ğ‘…ğ‘’ğ¿ğ‘ˆğ‘§= á‰Š0,
ğ‘§< 0
ğ‘§,
ğ‘§â‰¥0
= max 0, ğ‘§
ğ‘…ğ‘’ğ¿ğ‘ˆ0 = 0
ğ‘…ğ‘’ğ¿ğ‘ˆğ‘§= ğ‘§
ğ‘…ğ‘’ğ¿ğ‘ˆâˆ’ğ‘§= 0
for (ğ‘§â‰«0)
Rectified Linear Unit (ReLU)
â€œLeakyâ€ Rectified Linear Unit (ReLU)
ğ¿ğ‘…ğ‘’ğ¿ğ‘ˆğ‘§= á‰Šğ›¼ğ‘§,
ğ‘§< 0
ğ‘§,
ğ‘§â‰¥0
= max ğ›¼ğ‘§, ğ‘§
for (ğ›¼< 1)
ğ¿ğ‘…ğ‘’ğ¿ğ‘ˆ0 = 0
ğ¿ğ‘…ğ‘’ğ¿ğ‘ˆğ‘§= ğ‘§
ğ¿ğ‘…ğ‘’ğ¿ğ‘ˆâˆ’ğ‘§= âˆ’ğ›¼ğ‘§
for (ğ‘§â‰«0)
â€œLeakyâ€ Rectified Linear Unit (ReLU)
What next?
â€¢
We now know how to make a single u