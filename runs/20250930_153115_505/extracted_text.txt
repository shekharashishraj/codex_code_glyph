Quiz 1 
1.​Select the correct option: GPT-3 is a ____-gram model.​
​
(15 marks) 
a.. 1024 
b. 2048 
c.  4096 
d. 8192 
2.​In one line, explain why hallucinations can still occur in LLMs despite using 
advanced prompting techniques like Chain of Thought (CoT).​
(15 marks) 
3.​In one sentence, explain how Self-Verification helps reduce factual 
errors in an LLM’s output. ​​
​
​
​
​
​
​
(15 marks) 
4.​True or False: Advanced prompting techniques (such as Chain of Thought 
and Self-Ask) can enhance an LLM’s reasoning but may also amplify 
biases from training data, leading to more persuasive yet incorrect 
responses when the initial reasoning is flawed. Provide a one-line 
explanation.​
​
​
​
​
​
​
​
​
​
(15 marks) 
5.​Match the following ​ ​
​
​
​
​
​
​
​
(40 marks) 
 
 
Prompting Technique 
Primary Function/Characteristic 
a.​Zero-Shot CoT 
1.​Prompts the model to create a variety of 
reasoning paths on its own, then integrate the 
best results 
b.​Auto-CoT 
2. Asks follow-up questions to refine the model’s 
understanding and its final output 
c.​Self-Ask 
3. Provides a step-by-step explanation in one 
prompt without any additional examples 
d.​Program of Thoughts 
(PoT) 
4. Uses code-like structures or “scripts” to break 
down and solve tasks methodically 
e.​Tree of Thoughts 
5. Explores branching pathways to evaluate 
multiple partial solutions hierarchically 
 
Answers: 
1.​2048 
2.​Because LLMs rely on probabilistic text generation and incomplete 
training data, they can confidently generate factually incorrect 
statements, leading to hallucinations despite structured reasoning 
prompts.  
3.​Self-verification has the model revisit or question its own answer, 
checking for inconsistencies or factual inaccuracies before finalizing 
the response.  
4.​True - Because LLMs rely on probabilistic generation and may contain 
inherent biases, any technique that structures or extends the model’s 
reasoning (e.g., CoT, Self-Ask) can produce more detailed—but 
potentially biased or factually incorrect—responses if the foundational 
assumptions or data are flawed (incorrect).  
5.​Matchings: 
a.​Zero-Shot COT → 3 
b.​Auto-CoT → 1 
c.​Self-ask → 2 
d.​Program of Thoughts → 4 
e.​Tree of Thoughts → 5