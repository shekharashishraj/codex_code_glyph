CSE 576: Topics in Natural Language Processing
Dr. Vivek Gupta
Spring 2025
02/03/2025
Bayesian Networks
2
ğŸ“Œ Why Probability in NLP?
â—
Used for predicting next words, resolving ambiguity in text. Example: "I ate a 
cherry" is more likely than "Eye eight uh Jerry".
ğŸ“Œ N-Gram Language Models: 
â—
Estimate the probability of each word given its prior context.
â—
Markov Assumption: Future state depends only on the previous N-1 states.
â—
Unigram, bigrams, trigrams etc. 
ğŸ“Œ Evaluation of Language Models
â—
Perplexity: Measures how well a model fits test data.
â—
Lower perplexity = better language model.
Recap: N-grams
3
ğŸ“Œ Challenges with N-Gram Models
â—
Data sparsity: MLE assigns zero probability to unseen words.
â—
Smoothing techniques: Laplace, Good-Turing, Backoff, Kneser-Ney.
â—
Long-distance dependencies: N-grams struggle to model syntactic and semantic 
dependencies.
ğŸ“Œ Takeaway:
â—
N-grams work well for local dependencies but fail for longer contexts.
â—
We need a better way to model probabilities beyond local context!
â—
ğŸ”œ Enter Bayesian Networks!
Recap: N-grams
ğŸ“Œ Limitations of N-Grams
ğŸš§ Fixed-size context: Cannot model dependencies across long distances.
ğŸš§ Ignores uncertainty: Doesn't factor in external knowledge.
ğŸ“Œ Example: Long-Distance Dependencies
ğŸ’¬ Sentence: "The computer that crashed yesterday was running an update."
â—
N-grams might incorrectly predict "The computer that crashed yesterday were 
running an update."
â—
Why? "Was" depends on "computer," but the word "yesterday" separates them.
âœ… Bayesian Networks can capture dependencies between distant words!
ğŸ“Œ Example: Word Sense Disambiguation (external knowledge)
ğŸ’¬ Sentence 1: "The bank approved my loan."
ğŸ’¬ Sentence 2: "I sat by the river bank."
â—
N-grams only predict based on previous words, ignoring meaning.
â—
âœ… Bayesian Networks can infer the correct sense of "bank" using context!
Why Do We Need Bayesian Networks? - Where N-Grams Fall Short
Example: Handling Noisy Input (Speech Recognition & OCR Errors)
ğŸ“Œ Why N-Grams Struggle with Noisy Data
â—
N-grams assume clean input and struggle to handle spelling variations, typos, or 
speech recognition errors.
â—
They only consider local word probabilities without higher-level reasoning.
ğŸ“Œ Example: Speech Recognition Failure
ğŸ¤ User says: "I need to book a flight to Nice." (city in France)
ğŸ”„ Speech recognition system transcribes: "I need to book a flight to niece."
âŒ N-gram model: "Niece" is more probable than "Nice" in general, so it chooses the 
wrong word.
âœ… Bayesian Networks:
â—
Uses contextual dependencies (e.g., "flight" â†’ likely a city, not a relative).
â—
Adjusts probabilities dynamically based on the sentence structure.
â—
Predicts "Nice" as the correct word given the travel-related context.
Why Do We Need Bayesian Networks? - Where N-Grams Fall Short
ğŸ“Œ Example: Optical Character Recognition (OCR) Errors
ğŸ“ Scanned document says: "He loves reading b00ks."
ğŸ”„ OCR misreads: "He loves reading looks."
âŒ N-grams: "Looks" is more common than "books," so it accepts the incorrect word.
âœ… Bayesian Networks:
â—
Uses semantic dependencies to check if the corrected word fits the sentence context.
â—
Determines that "books" is more likely than "looks" after "reading."
Why Do We Need Bayesian Networks? - Where N-Grams Fall Short
ğŸ”— Graph-Based Representation:
â—
Unlike N-grams, which use linear dependencies, 
â—
Bayesian Networks use a Directed Acyclic Graph (DAG) to capture relationships between 
variables.
â—
Each node represents a random variable (e.g., word, symptom, POS tag).
â—
Each edge represents a probabilistic dependency (e.g., "flu" â†’ "fever").
ğŸ“Š Models Joint Probability Efficiently:
â—
Bayesian Networks factorize the full joint probability distribution, reducing computation.
â—
Instead of storing massive probability tables, they break it down into conditional probability 
tables (CPTs).
ğŸ’¡ Captures Conditional Dependencies:
â—
Context-aware predictions â†’ Takes into account multiple factors, unlike N-grams, which rely 
only on fixed-length history.